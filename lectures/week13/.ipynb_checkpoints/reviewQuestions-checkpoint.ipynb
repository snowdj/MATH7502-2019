{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Review Exercises on Units 1-5\n",
    "\n",
    "*Many taken/adapted from \\[VMLS\\] or \\[LALFD\\]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Consider a function $f: {\\mathbb R}^n \\to {\\mathbb R}^m$ that is linear. Show that there exists a unique $m \\times n$ matrix $A$ such that $f(x) = A x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Consider the function $f: {\\mathbb R}^2 \\to {\\mathbb R}$ with $f(x) = x_1 x_2 + e^{x_1^2}$. Find the gradient of $f$ and obtain the linear approximation at $x=(1,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Verify that for any affine function $f: {\\mathbb R}^n \\to {\\mathbb R}$ with $f(x) = a^T x + b$:\n",
    "\n",
    "$$\n",
    "f(x) = f(0) + \\sum_{i=1}^n x_i \\big(f(e_i) - f(0)\\big).\n",
    "$$\n",
    "\n",
    "Generalize to an affine function $f: {\\mathbb R}^n \\to {\\mathbb R}^m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Show: $||x+y|| = \\sqrt{||x||^2 + 2 x^T y + ||y||^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) Remember that \n",
    "$$\n",
    "\\text{rms}(x) = \\frac{||x||}{\\sqrt{n}}\n",
    "\\qquad\n",
    "\\text{avg}(x) = \\frac{{\\mathbf 1}^T x}{n}\n",
    "\\qquad\n",
    "\\text{std}(x) =  \\frac{||x - n^{-1} {\\mathbf 1}^T x {\\mathbf 1}||}{\\sqrt{n}}.\n",
    "$$\n",
    "\n",
    "Now show that,\n",
    "$$\n",
    "\\text{std}(x)^2 = \\text{rms}(x)^2 - \\text{avg}(x)^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " (6) Prove the Cauchy-Schwarz inequality: $|a^T b| \\le ||a||~ ||b||$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7) Let $a$ and $b$ be two different $n$-vectors. The line passing through $a$ and $b$ is given by the set of vectors of the form $(1-\\theta)a + \\theta b$ where $\\theta$ is a scalar that determines the particular point on the line. Now let $x$ be any $n$-vector. Find a formula for the point $p$ on the line that is closest to $x$. This point is called the projection of $x$ onto the line. Show that $(p-x) \\perp (a-b)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(8) Let $x_1,\\ldots,x_L$ be a collection of $n$-vectors. Consider,\n",
    "$$\n",
    "J(z) = \\sum_{i=1}^L ||x_i - z||^2.\n",
    "$$\n",
    "Find the vector $z$ that minimizes $J(z)$ and prove your result. Describe how this relates to the $k$-means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(9) Prove that an orthonormal (or even orthogonal) set of vectors is linearly indepndent and say what this says about the number of vectors in the set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(10) Say you have two different basia (plural of basis) for ${\\mathbb R}^n$. The basis ${\\cal B}_1$ is orthonomral and the basis ${\\cal B}_2$ is not. You have a vector $x$ and want to expand $x$ in two different ways. Once in terms of ${\\cal B}_1$ and once in terms of ${\\cal B}_2$. How would you go about finding such expansions in an efficient manner. Which is easier/quicker?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(11) Carry out the Gram-Schmidt algorithm on the vectors $a_1 = (1,2,3)$, $a_2 = (1,0,3)$, $a_3 = (3,2,6)$ and $a_4 = (1,1,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(12) Repeat now without $a_4$ and with $a_3 = (2,2,6)$ instead. Suggest how to modify the algorithm so that in cases such as the first, it won't terminate but rather flag the vectors $a_3$ and $a_4$ to be linear combinations of the previous vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(13) Say we run the Gram-Schmidt algorithm twice. First on a given independent set of vectors $a_1,\\ldots,a_k$ which gives the vectors $q_1,\\ldots,q_k$. Then we run again on $q_1,\\ldots,q_k$ as input to obtain $z_1,\\ldots,z_k$. What can you say about $z_1,\\ldots,z_k$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(14) Take $A$ as an $m \\times n$ matrix. Say you want to exchange rows $i$ and $j$ of $A$ by multiplying by a matrix $P$. What is $P$ and would you left-multiply or right-multiply? What if you wanted to exchange collumns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(15) Let $A$ be an $m \\times n$ matrix. Consider the stacked matrix $S$ defined by,\n",
    "\n",
    "$$\n",
    "S = \\left[\\begin{matrix}\n",
    "A \\\\ I \n",
    "\\end{matrix}\\right].\n",
    "$$\n",
    "\n",
    "When does $S$ have lineary independent collumns? When does $S$ have linearly indepdnent rows? Your answer can depend on $m$, $n$ or whether or not $A$ has linearly indpendent columns or rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(16) Let $f: {\\mathbb R}^p \\to {\\mathbb R}^m$ and $g: {\\mathbb R}^n \\to {\\mathbb R}^p$ and define $h(x) = f\\big(g(x)\\big)$. The is $h: {\\mathbb R}^n \\to {\\mathbb R}^m$ and,\n",
    "$$\n",
    "h(x) = f(g_1(x),\\ldots,g_p(x)).\n",
    "$$\n",
    "From the chain rule, we have that, \n",
    "$$\n",
    "\\frac{d h_i}{d x_j} (z) = \\sum_{\\ell=1}^p \\frac{d f_i}{d y_\\ell}\\big(g(z)\\big) \\frac{d g_\\ell}{d x_j}(z).\n",
    "\\qquad\n",
    "\\text{for}~~\n",
    "i=1,\\ldots,m,~~j=1,\\ldots,n.\n",
    "$$\n",
    "\n",
    "Show that this can be described concisely via Jacobian matrices as follows:\n",
    "$$\n",
    "Dh(z) = Df\\big(g(z)\\big) Dg(z).\n",
    "$$\n",
    "\n",
    "Now show that the first or Taylor approximation of $h$ at $z$ can be written as ,\n",
    "$$\n",
    "\\hat{h}(x) = f\\big(g(z)\\big) + Df\\big(g(z)\\big) Dg(z) ~(x-z)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(17) Suppose the columns of $A$ are orthonormal. Show that $||Ax|| = ||x||$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(18) Let $a_1,\\ldots,a_n$ be the collumns of the $m \\times n$ matrix $A$. Suppose the collumns all have norm $1$ and for $i \\neq j$ the angle between $a_i$ and $a_j$ is $60$ degrees. What can be said about the gram matrix $G = A^T A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(19) A student says that for any square matrix $A$,\n",
    "$$\n",
    "(A+I)^3 = A^3 + 3 A^2 + 3 A + I.\n",
    "$$\n",
    "Is she right? Explain why? Can you generalize this claim?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(20) Let $U$ and $V$ be two orthogonal $n \\times n$ matrices. Show that the matrix $UV$ is orthogonal. Show also that the $2n \\times 2n$ matrix,\n",
    "$$\n",
    "W =\n",
    "\\frac{1}{\\sqrt{2}}\n",
    "\\left[\\begin{matrix}\n",
    "U & U \\\\\n",
    "V & -V \n",
    "\\end{matrix}\\right]\n",
    "$$\n",
    "is orthogonal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
